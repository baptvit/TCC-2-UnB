{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-lime-pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7-OtCNe_tTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os, json\n",
        "from torchvision import models, transforms\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "from skimage.segmentation import mark_boundaries"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCpQAYqCjvZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWDSYTJJhZD6",
        "colab_type": "text"
      },
      "source": [
        "## Download lime with pip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODPvQwfbhedw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "320b0b9d-f318-473b-d5ab-3e1f6e5d951b"
      },
      "source": [
        "!pip install lime"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/72/4be533df5151fcb48942515e95e88281ec439396c48d67d3ae41f27586f0/lime-0.1.1.36.tar.gz (275kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 28.7MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 3.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 4.7MB/s eta 0:00:01\r\u001b[K     |████▊                           | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 3.7MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 81kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92kB 6.2MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 102kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 112kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 122kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 133kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 143kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 153kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 163kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 174kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 184kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 194kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 204kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 225kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 235kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 245kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 256kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 266kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lime) (1.17.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lime) (1.3.3)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from lime) (0.21.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from lime) (3.1.2)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.6/dist-packages (from lime) (0.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->lime) (0.14.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (2.6.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (2.4.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (0.10.0)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (4.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (2.4)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (2.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->lime) (42.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->lime) (1.12.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.3.0->scikit-image>=0.12->lime) (0.46)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.12->lime) (4.4.1)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.1.1.36-cp36-none-any.whl size=284191 sha256=5d9ab36a5fefe740bd41fde04e94562b84871d1b474ac4deeb561ca6331148a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/2f/25/4b2127822af5761dab9a27be52e175105772aebbcbc484fb95\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.1.1.36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBP8jmGd_ytH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from lime import lime_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ1raYa3hpoB",
        "colab_type": "text"
      },
      "source": [
        "## Using Lime with Pytorch\n",
        "\n",
        "Based on a oficial repo's notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zoj-HX1ah2_o",
        "colab_type": "text"
      },
      "source": [
        "Image preview "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd-WcjOqhRla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_image(path):\n",
        "  '''\n",
        "  Function to make a simple vizualization\n",
        "  '''\n",
        "    with open(os.path.abspath(path), 'rb') as f:\n",
        "        with Image.open(f) as img:\n",
        "            return img.convert('RGB') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9zRLCx6hUoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# img = get_image(IMG_PATH)\n",
        "# plt.imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODBpbbGViait",
        "colab_type": "text"
      },
      "source": [
        "We need to convert this image to Pytorch tensor and also apply transformations in order to feed our pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REbSSsiYiZmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_input_transform():\n",
        "  '''\n",
        "  Pre transformations applied on the image\n",
        "  '''\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])       \n",
        "    transf = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])    \n",
        "\n",
        "    return transf\n",
        "\n",
        "def get_input_tensors(img):\n",
        "  '''\n",
        "  Convert image to feed the model\n",
        "  '''\n",
        "    transf = get_input_transform()\n",
        "    # unsqeeze converts single image to batch of 1\n",
        "    return transf(img).unsqueeze(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al6PFhe2jCD0",
        "colab_type": "text"
      },
      "source": [
        "### Load the skin lession model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIbgCyeKiYSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PRE_MODEL_DIR='/content/gdrive/My Drive/UnB/TCC-1/TCC1-1-dataset-final/restnet_model152_trained_exp7.pt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phVn5ielj2G3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name='resnet'\n",
        "num_classes = 9\n",
        "feature_extract = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guartz4Kj9T7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr8LOzXLj-6b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c0b565a1-dba9-44c8-91da-7390e7ba566d"
      },
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet152\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet152(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "    \n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "model, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/checkpoints/resnet152-b121ed2d.pth\n",
            "100%|██████████| 230M/230M [00:05<00:00, 45.3MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsU-oGmekG5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if train_on_gpu:\n",
        "    state = torch.load(PRE_MODEL_DIR)\n",
        "else:\n",
        "    state = torch.load(PRE_MODEL_DIR, map_location='cpu')\n",
        "\n",
        "# Loading weights in restnet architecture\n",
        "model.load_state_dict(state['state_dict'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdNdwv9ikQKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes_skin = state['class_to_idx']\n",
        "classes_skin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN_rn2-1kx5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx2label = [] \n",
        "cls2label = {}\n",
        "cls2idx = {}\n",
        "\n",
        "idx2label = [classes_skin[str(k)][1] for k in range(len(classes_skin))]\n",
        "cls2label = {classes_skin[str(k)][0]: classes_skin[str(k)][1] for k in range(len(classes_skin))}\n",
        "cls2idx = {classes_skin[str(k)][0]: k for k in range(len(classes_skin))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDWvM3LWlWet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_t = get_input_tensors(img)\n",
        "model.eval()\n",
        "logits = model(img_t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9CP1DiglaO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "probs = F.softmax(logits, dim=1)\n",
        "probs5 = probs.topk(5)\n",
        "tuple((p,c, idx2label[c]) for p, c in zip(probs5[0][0].detach().numpy(), probs5[1][0].detach().numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNfYR37blidh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_pil_transform(): \n",
        "    transf = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.CenterCrop(224)\n",
        "    ])    \n",
        "\n",
        "    return transf\n",
        "\n",
        "def get_preprocess_transform():\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])     \n",
        "    transf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])    \n",
        "\n",
        "    return transf    \n",
        "\n",
        "pill_transf = get_pil_transform()\n",
        "preprocess_transform = get_preprocess_transform()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOFrm-eKlmzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_predict(images):\n",
        "    model.eval()\n",
        "    batch = torch.stack(tuple(preprocess_transform(i) for i in images), dim=0)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    batch = batch.to(device)\n",
        "    \n",
        "    logits = model(batch)\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    return probs.detach().cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EW36iNUluNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred = batch_predict([pill_transf(img)])\n",
        "test_pred.squeeze().argmax()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0wqrT7Qlyxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "explainer = lime_image.LimeImageExplainer()\n",
        "explanation = explainer.explain_instance(np.array(pill_transf(img)), \n",
        "                                         batch_predict, # classification function\n",
        "                                         top_labels=5, \n",
        "                                         hide_color=0, \n",
        "                                         num_samples=1000) # number of images that will be sent to classification function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn2Tf8EJmPdI",
        "colab_type": "text"
      },
      "source": [
        "Let's use mask on image and see the areas that are encouraging the top prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmE5wPc6l1aj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)\n",
        "img_boundry1 = mark_boundaries(temp/255.0, mask)\n",
        "plt.imshow(img_boundry1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDch_PpumS8G",
        "colab_type": "text"
      },
      "source": [
        "Let's turn on areas that contributes against the top prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djuTysw9mTRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=10, hide_rest=False)\n",
        "img_boundry2 = mark_boundaries(temp/255.0, mask)\n",
        "plt.imshow(img_boundry2)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}